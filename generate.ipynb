{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows how one can generate text given a prompt and some hyperparameters, using either minGPT or huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orfeu/Documents/documents/info_perso/minGPT/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from mingpt.model import GPT\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.bpe import BPETokenizer\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mingpt = True # use minGPT or huggingface/transformers model?\n",
    "model_type = 'gpt2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    }
   ],
   "source": [
    "if use_mingpt:\n",
    "    model = GPT.from_pretrained(model_type)\n",
    "else:\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "    model.config.pad_token_id = model.config.eos_token_id # suppress a warning\n",
    "\n",
    "# ship model to device and set to eval mode\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
    "        \n",
    "    # tokenize the input prompt into integer input sequence\n",
    "    if use_mingpt:\n",
    "        tokenizer = BPETokenizer()\n",
    "        if prompt == '':\n",
    "            # to create unconditional samples...\n",
    "            # manually create a tensor with only the special <|endoftext|> token\n",
    "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
    "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "        else:\n",
    "            x = tokenizer(prompt).to(device)\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "        if prompt == '': \n",
    "            # to create unconditional samples...\n",
    "            # huggingface/transformers tokenizer special cases these strings\n",
    "            prompt = '<|endoftext|>'\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        x = encoded_input['input_ids']\n",
    "    \n",
    "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    # forward the model `steps` times to get samples, in a batch\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('-'*80)\n",
    "        print(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test(use_mingpt, model_type, prompt, device, model, steps=20, do_sample=True):\n",
    "    \n",
    "    empty_prompt = False\n",
    "    # tokenize the input prompt into integer input sequence\n",
    "    if use_mingpt:\n",
    "        tokenizer = BPETokenizer()\n",
    "        if prompt == '':\n",
    "            empty_prompt = True\n",
    "            pass\n",
    "        else:\n",
    "            x = tokenizer(prompt).to(device)\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "        if prompt == '': \n",
    "            empty_prompt = True\n",
    "            pass\n",
    "        else:\n",
    "            encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "            x = encoded_input['input_ids']\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not empty_prompt:\n",
    "        # forward the model `steps` times to get samples, in a batch\n",
    "        y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "\n",
    "        out = tokenizer.decode(y[0].cpu())\n",
    "    else:\n",
    "        out = \"no prompt given, plese retry following the instructions above\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"HR recommendation systems must continuously adapt to evolving candidate profiles and job postings.\\\n",
    "    Static, batch-trained models struggle to stay up-to-date and often fail to maintain relevance over time without costly retraining.\\\n",
    "    This PhD project explores how continual learning (CL) and online learning to rank (OLTR) can enable adaptive, \\\n",
    "    scalable HR systems built on structured JSON data—including parsed resumes and job descriptions. \\\n",
    "    Key research questions include: Incremental adaptation, Real-time ranking, Domain-specific embeddings, \\\n",
    "    Conducted in partnership with HrFlow.ai, this research is anchored in real-world, structured HR datasets and production systems.\\\n",
    "    Expected contributions include new algorithms for lifelong ranking and dynamic representation learning tailored to HR applications.\\n\\n\\\n",
    "    list of requirements: automnomy, creativity, strong coding skills in python, machine learning, deep learning, NLP, information retrieval, \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt_test_1 = \"\"\n",
    "input_prompt_test_2 = \"Hey, I am\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_test\u001b[49m(use_mingpt=use_mingpt, model_type=model_type, prompt=input_prompt_test_1, device=device, model=model, steps=\u001b[32m20\u001b[39m, do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_test' is not defined"
     ]
    }
   ],
   "source": [
    "generate_test(use_mingpt=use_mingpt, model_type=model_type, prompt=input_prompt_test_1, device=device, model=model, steps=20, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey, I am here to take care of you. It doesn't matter if you're a bad guy, a traitor\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_test(use_mingpt=use_mingpt, model_type=model_type, prompt=input_prompt_test_2, device=device, model=model, steps=20, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "HR recommendation systems must continuously adapt to evolving candidate profiles and job postings.    Static, batch-trained models struggle to stay up-to-date and often fail to maintain relevance over time without costly retraining.    This PhD project explores how continual learning (CL) and online learning to rank (OLTR) can enable adaptive,     scalable HR systems built on structured JSON data—including parsed resumes and job descriptions.     Key research questions include: Incremental adaptation, Real-time ranking, Domain-specific embeddings,     Conducted in partnership with HrFlow.ai, this research is anchored in real-world, structured HR datasets and production systems.    Expected contributions include new algorithms for lifelong ranking and dynamic representation learning tailored to HR applications.\n",
      "\n",
      "    list of requirements: automnomy, creativity, strong coding skills in python, machine learning, deep learning, NLP, information retrieval,  solving problems through deep learning and learning-\n"
     ]
    }
   ],
   "source": [
    "generate(prompt=input_prompt, num_samples=1, steps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
